{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nehal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/nehal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nehal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import math\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer\n",
    "import joblib\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from natsort import natsorted\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "#initialize the variables neede\n",
    "path =Path(\"./Humor,Hist,Media,Food\")\n",
    "\n",
    "\n",
    "global doc_token_list, doc_name , doc_df_preprocessed,file_names\n",
    "doc_token_list = []\n",
    "doc_df_preprocessed = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "  text = \" \".join(text)\n",
    "  text = re.sub(r'[-!$%^&*#()_+|@\\]\\[~=`{}\\\\:\"\\';<>?,.\\/]','',text)\n",
    "  text = re.sub(r'[0-9]+','',text)\n",
    "  text = text.strip()\n",
    "\n",
    "  return text.split(\" \")\n",
    "\n",
    "def tokenize(text):\n",
    "  text = word_tokenize(text)\n",
    "  return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [i for i in text if not i in stop_words]\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_whitespace(text):\n",
    "  temp = list()\n",
    "  for i in range(len(text)):\n",
    "    if len(text[i].split()):\n",
    "      temp.append(\"\".join(text[i].split()))\n",
    "  return temp\n",
    "\n",
    "\n",
    "def preProcess(text):\n",
    "  text=text.lower()\n",
    "  text = tokenize(text)\n",
    "  text = remove_punctuation(text)\n",
    "  text = remove_stopwords(text)\n",
    "  text = remove_whitespace(text)\n",
    "  return text\n",
    "\n",
    "doc_name = []\n",
    "\n",
    "file_names = natsorted(os.listdir(path))\n",
    "\n",
    "def tokenizeDocs(path):\n",
    "  for file in file_names:\n",
    "    f = open(str(path)+\"/\"+file,'r', encoding =\"utf8\", errors =\"surrogateescape\")\n",
    "    # split the lines for any multiple new lines\n",
    "    data = f.read()\n",
    "    pre_processed_data = preProcess(data)\n",
    "    \n",
    "    doc_token_list.append(pre_processed_data)\n",
    "    doc_name.append(file)\n",
    "\n",
    "\n",
    "tokenizeDocs(path)\n",
    "joblib.dump(doc_token_list,'doc_token_list.joblib')\n",
    "doc_df_preprocessed= pd.DataFrame(list(zip(doc_name,doc_token_list)),columns=['doc_name','doc_token'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_token_list = joblib.load('doc_token_list.joblib')\n",
    "# doc_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df_preprocessed['length'] = doc_df_preprocessed.apply(lambda row: len(row['doc_token']),axis=1)\n",
    "doc_df_preprocessed.set_index('doc_name',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_df_preprocessed.set_index('doc_name',inplace=True)\n",
    "joblib.dump(doc_df_preprocessed,'doc_df_preprocessed.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_token</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1st_aid.txt</th>\n",
       "      <td>[herbalherbst, aidcalendulacomfreyremediessick...</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a-team</th>\n",
       "      <td>[uunet, csutexasedu, usc, ucsd, ucbvax, caewis...</td>\n",
       "      <td>3082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_fish_c.apo</th>\n",
       "      <td>[murph, buscardfidonetorg, brian, murphy, subj...</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_tv_t-p.com</th>\n",
       "      <td>[survey, results, computer, use, fans, alttvtw...</td>\n",
       "      <td>901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbott.txt</th>\n",
       "      <td>[abbott, costello, first, abbott, well, costel...</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zen.txt</th>\n",
       "      <td>[young, studious, monk, went, teacher, said, t...</td>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zgtoilet.txt</th>\n",
       "      <td>[zero, gravity, toilet, passengers, required, ...</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zodiac.hum</th>\n",
       "      <td>[capricorn, decjan, consservative, afraid, tak...</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zucantom.sal</th>\n",
       "      <td>[zucchini, tomato, salad, category, salads, si...</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuccmush.sal</th>\n",
       "      <td>[zucchini, mushroom, salad, category, salads, ...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1133 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      doc_token  length\n",
       "doc_name                                                               \n",
       "1st_aid.txt   [herbalherbst, aidcalendulacomfreyremediessick...     217\n",
       "a-team        [uunet, csutexasedu, usc, ucsd, ucbvax, caewis...    3082\n",
       "a_fish_c.apo  [murph, buscardfidonetorg, brian, murphy, subj...     159\n",
       "a_tv_t-p.com  [survey, results, computer, use, fans, alttvtw...     901\n",
       "abbott.txt    [abbott, costello, first, abbott, well, costel...     671\n",
       "...                                                         ...     ...\n",
       "zen.txt       [young, studious, monk, went, teacher, said, t...     606\n",
       "zgtoilet.txt  [zero, gravity, toilet, passengers, required, ...     452\n",
       "zodiac.hum    [capricorn, decjan, consservative, afraid, tak...     242\n",
       "zucantom.sal  [zucchini, tomato, salad, category, salads, si...     107\n",
       "zuccmush.sal  [zucchini, mushroom, salad, category, salads, ...      86\n",
       "\n",
       "[1133 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_df_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def relevance(query):\n",
    "  import joblib\n",
    "  # print(\"Jaccard_coeff\")\n",
    "  query_token_list = preProcess(query)\n",
    "  \n",
    "  doc_df_preprocessed = joblib.load('doc_df_preprocessed.joblib')\n",
    "\n",
    "  \"\"\"Calculate the intersection and union with the query tokens and each document\"\"\"\n",
    "  \n",
    "  doc_df_preprocessed['union'] = doc_df_preprocessed.apply(lambda row:list(set(row['doc_token']) | set(query_token_list)),axis=1 )\n",
    "  doc_df_preprocessed['intersection'] = doc_df_preprocessed.apply(lambda row:list(set(row['doc_token']) & set(query_token_list)),axis=1 )\n",
    "  doc_df_preprocessed['jaccard_coeff'] = doc_df_preprocessed.apply(lambda row:len(row['intersection'])/len(row['union']),axis=1 )\n",
    "  # doc_df_preprocessed.set_index('doc_name')\n",
    "  return doc_df_preprocessed\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------- Testing for JACCARD COEFFICIENT -------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top5 relevant docs--->\n",
      "               jaccard_coeff\n",
      "doc_name                   \n",
      "languag.jok        0.015873\n",
      "temphell.jok       0.015152\n",
      "calif.hum          0.014706\n",
      "gaiahuma           0.012821\n",
      "hedgehog.txt       0.012579\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" Find the relevance with the query using the dataframe\"\"\"\n",
    "\n",
    "doc_df_preprocessed = relevance('jokes rules attends received')\n",
    "\n",
    "\"\"\" Top 5 \"\"\"\n",
    "top5 = doc_df_preprocessed.nlargest(5, ['jaccard_coeff'])\n",
    "print('top5 relevant docs--->\\n',top5[['jaccard_coeff']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doc_token_list\n",
    "\n",
    "len(np.unique(np.array(doc_df_preprocessed.iloc[0,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import unicodedata\n",
    "\"\"\"Make a corpus word set which contains all the unique words in the dataset\"\"\"\n",
    "global corpus_words\n",
    "corpus_words = set()\n",
    "for i in range(len(doc_token_list)):\n",
    "    for word in doc_token_list[i]:\n",
    "        corpus_words.add(word)\n",
    "# len(corpus_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df = pd.DataFrame(0, index=np.arange(len(corpus_words)), columns=doc_name)\n",
    "tf_df.index = corpus_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st_aid.txt</th>\n",
       "      <th>a-team</th>\n",
       "      <th>a_fish_c.apo</th>\n",
       "      <th>a_tv_t-p.com</th>\n",
       "      <th>abbott.txt</th>\n",
       "      <th>aboutada.txt</th>\n",
       "      <th>acetab1.txt</th>\n",
       "      <th>aclamt.txt</th>\n",
       "      <th>acne1.txt</th>\n",
       "      <th>acronym.lis</th>\n",
       "      <th>...</th>\n",
       "      <th>yjohncse.hum</th>\n",
       "      <th>yogisays.txt</th>\n",
       "      <th>yogurt.asc</th>\n",
       "      <th>yuban.txt</th>\n",
       "      <th>yuppies.hum</th>\n",
       "      <th>zen.txt</th>\n",
       "      <th>zgtoilet.txt</th>\n",
       "      <th>zodiac.hum</th>\n",
       "      <th>zucantom.sal</th>\n",
       "      <th>zuccmush.sal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aviles</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unsteadily</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polio</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genovese</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creamsicle</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chest</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pirated</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>halleyuucp</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glycerine</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>platinum</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73606 rows Ã— 1133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1st_aid.txt  a-team  a_fish_c.apo  a_tv_t-p.com  abbott.txt  \\\n",
       "aviles                0       0             0             0           0   \n",
       "unsteadily            0       0             0             0           0   \n",
       "polio                 0       0             0             0           0   \n",
       "genovese              0       0             0             0           0   \n",
       "creamsicle            0       0             0             0           0   \n",
       "...                 ...     ...           ...           ...         ...   \n",
       "chest                 0       0             0             0           0   \n",
       "pirated               0       0             0             0           0   \n",
       "halleyuucp            0       0             0             0           0   \n",
       "glycerine             0       0             0             0           0   \n",
       "platinum              0       0             0             0           0   \n",
       "\n",
       "            aboutada.txt  acetab1.txt  aclamt.txt  acne1.txt  acronym.lis  \\\n",
       "aviles                 0            0           0          0            0   \n",
       "unsteadily             0            0           0          0            0   \n",
       "polio                  0            0           0          0            0   \n",
       "genovese               0            0           0          0            0   \n",
       "creamsicle             0            0           0          0            0   \n",
       "...                  ...          ...         ...        ...          ...   \n",
       "chest                  0            0           0          0            0   \n",
       "pirated                0            0           0          0            0   \n",
       "halleyuucp             0            0           0          0            0   \n",
       "glycerine              0            0           0          0            0   \n",
       "platinum               0            0           0          0            0   \n",
       "\n",
       "            ...  yjohncse.hum  yogisays.txt  yogurt.asc  yuban.txt  \\\n",
       "aviles      ...             0             0           0          0   \n",
       "unsteadily  ...             0             0           0          0   \n",
       "polio       ...             0             0           0          0   \n",
       "genovese    ...             0             0           0          0   \n",
       "creamsicle  ...             0             0           0          0   \n",
       "...         ...           ...           ...         ...        ...   \n",
       "chest       ...             0             0           0          0   \n",
       "pirated     ...             0             0           0          0   \n",
       "halleyuucp  ...             0             0           0          0   \n",
       "glycerine   ...             0             0           0          0   \n",
       "platinum    ...             0             0           0          0   \n",
       "\n",
       "            yuppies.hum  zen.txt  zgtoilet.txt  zodiac.hum  zucantom.sal  \\\n",
       "aviles                0        0             0           0             0   \n",
       "unsteadily            0        0             0           0             0   \n",
       "polio                 0        0             0           0             0   \n",
       "genovese              0        0             0           0             0   \n",
       "creamsicle            0        0             0           0             0   \n",
       "...                 ...      ...           ...         ...           ...   \n",
       "chest                 0        0             0           0             0   \n",
       "pirated               0        0             0           0             0   \n",
       "halleyuucp            0        0             0           0             0   \n",
       "glycerine             0        0             0           0             0   \n",
       "platinum              0        0             0           0             0   \n",
       "\n",
       "            zuccmush.sal  \n",
       "aviles                 0  \n",
       "unsteadily             0  \n",
       "polio                  0  \n",
       "genovese               0  \n",
       "creamsicle             0  \n",
       "...                  ...  \n",
       "chest                  0  \n",
       "pirated                0  \n",
       "halleyuucp             0  \n",
       "glycerine              0  \n",
       "platinum               0  \n",
       "\n",
       "[73606 rows x 1133 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df = pd.DataFrame(0, index=np.arange(len(corpus_words)), columns=doc_name)\n",
    "tf_df.index = corpus_words\n",
    "\n",
    "\"\"\"Iterate over each word document pair and fill the dataframe with 1 if the word is present in the document\"\"\"\n",
    "def termFreq_binary():\n",
    "    doc_df_preprocessed = joblib.load('doc_df_preprocessed.joblib')\n",
    "    for doc in doc_df_preprocessed.index:\n",
    "        for word in doc_df_preprocessed.loc[doc,'doc_token']:\n",
    "            tf_df.loc[word,doc] = 1\n",
    "            \n",
    "    joblib.dump(tf_df,'termFreqBinary.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf - raw count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df = pd.DataFrame(0, index=np.arange(len(corpus_words)), columns=doc_name)\n",
    "tf_df.index = corpus_words\n",
    "\n",
    "\"\"\"Iterate over each word document pair and fill the dataframe with the raw count of words\"\"\"\n",
    "def termFreqRaw():\n",
    "  doc_df_preprocessed = joblib.load('doc_df_preprocessed.joblib')\n",
    "  for doc in doc_df_preprocessed.index:\n",
    "    for word in doc_df_preprocessed.loc[doc,'doc_token']:\n",
    "      tf_df.loc[word,doc] = doc_df_preprocessed.loc[doc,'doc_token'].count(word)\n",
    "  joblib.dump(tf_df,'termFreqRaw.joblib')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf- termFreq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df = pd.DataFrame(0, index=np.arange(len(corpus_words)), columns=doc_name)\n",
    "tf_df.index = corpus_words\n",
    "\"\"\"Iterate over each word document pair and fill the dataframe with the raw count of words divided by the total number of terms\"\"\"\n",
    "\n",
    "def termFreq():\n",
    "  doc_df_preprocessed = joblib.load('doc_df_preprocessed.joblib')\n",
    "  for doc in doc_df_preprocessed.index:\n",
    "    for word in doc_df_preprocessed.loc[doc,'doc_token']:\n",
    "      tf_df.loc[word,doc] = doc_df_preprocessed.loc[doc,'doc_token'].count(word)/doc_df_preprocessed.loc[doc,'length']\n",
    "  joblib.dump(tf_df,'termFreq.joblib')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf - Log Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df = pd.DataFrame(0, index=np.arange(len(corpus_words)), columns=doc_name)\n",
    "tf_df.index = corpus_words\n",
    "\n",
    "\"\"\"Iterate over each word document pair and fill the dataframe with log(count of term in doc+1) / number of words in doc\"\"\"\n",
    "\n",
    "def termFreq_Log_norm():\n",
    "  doc_df_preprocessed = joblib.load('doc_df_preprocessed.joblib')\n",
    "  \n",
    "  for doc in doc_df_preprocessed.index:\n",
    "\n",
    "    for word in doc_df_preprocessed.loc[doc,'doc_token']:\n",
    "\n",
    "      tf_df.loc[word,doc] =np.log(doc_df_preprocessed.loc[doc,'doc_token'].count(word)+1)\n",
    "  joblib.dump(tf_df,'termFreqLogNorm.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-double normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_freq_doc, max_freq_doc1\n",
    "max_freq_doc={}\n",
    "max_freq_doc1={}\n",
    "\"\"\"Create a dictionary that contains the maximum term frequency\"\"\"\n",
    "def maxF():\n",
    "    doc_df_preprocessed = joblib.load('doc_df_preprocessed.joblib')\n",
    "    \"\"\" create a max frequency dictionary for every document\"\"\"\n",
    "  \n",
    "    for doc in doc_df_preprocessed.index:\n",
    "        for word in doc_df_preprocessed.loc[doc,'doc_token']:\n",
    "            max_freq_doc[word] = doc_df_preprocessed.loc[doc,'doc_token'].count(word)\n",
    "        \n",
    "        max_freq_doc1[doc] = max(max_freq_doc.values())\n",
    "\n",
    "maxF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1st_aid.txt': 6,\n",
       " 'a-team': 64,\n",
       " 'a_fish_c.apo': 64,\n",
       " 'a_tv_t-p.com': 64,\n",
       " 'abbott.txt': 91,\n",
       " 'aboutada.txt': 91,\n",
       " 'acetab1.txt': 91,\n",
       " 'aclamt.txt': 91,\n",
       " 'acne1.txt': 91,\n",
       " 'acronym.lis': 91,\n",
       " 'acronym.txt': 91,\n",
       " 'acronyms.txt': 132,\n",
       " 'adameve.hum': 132,\n",
       " 'adcopy.hum': 132,\n",
       " 'addrmeri.txt': 132,\n",
       " 'admin.txt': 132,\n",
       " 'adrian_e.faq': 132,\n",
       " 'ads.txt': 132,\n",
       " 'adt_miam.txt': 132,\n",
       " 'advrtize.txt': 132,\n",
       " 'aeonint.txt': 132,\n",
       " 'age.txt': 132,\n",
       " 'aggie.txt': 132,\n",
       " 'aids.txt': 91,\n",
       " 'airlines': 91,\n",
       " 'alabama.txt': 91,\n",
       " 'alcatax.txt': 91,\n",
       " 'alcohol.hum': 91,\n",
       " 'alflog.txt': 91,\n",
       " 'all_grai': 93,\n",
       " 'allfam.epi': 193,\n",
       " 'allusion': 193,\n",
       " 'amazing.epi': 193,\n",
       " 'ambrose.bie': 193,\n",
       " 'amchap2.txt': 193,\n",
       " 'analogy.hum': 193,\n",
       " 'aniherb.txt': 193,\n",
       " 'anim_lif.txt': 193,\n",
       " 'anime.cli': 193,\n",
       " 'anime.lif': 193,\n",
       " 'annoy.fascist': 193,\n",
       " 'anorexia.txt': 193,\n",
       " 'answers': 193,\n",
       " 'anthropo.stu': 193,\n",
       " 'antibiot.txt': 193,\n",
       " 'antimead.bev': 193,\n",
       " 'aphrodis.txt': 193,\n",
       " 'appbred.brd': 193,\n",
       " 'appetiz.rcp': 193,\n",
       " 'applepie.des': 193,\n",
       " 'apsaucke.des': 193,\n",
       " 'apsnet.txt': 193,\n",
       " 'arab.dic': 193,\n",
       " 'arcadian.txt': 193,\n",
       " 'argotdic.txt': 193,\n",
       " 'arnold.txt': 193,\n",
       " 'art-fart.hum': 193,\n",
       " 'arthriti.txt': 193,\n",
       " 'ateam.epi': 193,\n",
       " 'atherosc.txt': 193,\n",
       " 'atombomb.hum': 193,\n",
       " 'att.txt': 193,\n",
       " 'aussie.lng': 193,\n",
       " 'avengers.lis': 193,\n",
       " 'awespinh.sal': 193,\n",
       " 'ayurved.txt': 193,\n",
       " 'b12.txt': 193,\n",
       " 'b-2.jok': 193,\n",
       " 'back1.txt': 193,\n",
       " 'bad': 193,\n",
       " 'bad-d': 193,\n",
       " 'bad.jok': 193,\n",
       " 'badday.hum': 193,\n",
       " 'bagelope.txt': 193,\n",
       " 'bakebred.txt': 193,\n",
       " 'baklava.des': 193,\n",
       " 'banana01.brd': 193,\n",
       " 'banana02.brd': 193,\n",
       " 'banana03.brd': 193,\n",
       " 'banana04.brd': 193,\n",
       " 'banana05.brd': 193,\n",
       " 'bank.rob': 193,\n",
       " 'barney.cn1': 193,\n",
       " 'barney.txt': 207,\n",
       " 'basehead.txt': 207,\n",
       " 'batrbred.txt': 207,\n",
       " 'bb': 207,\n",
       " 'bbc_vide.cat': 207,\n",
       " 'bbh_intv.txt': 207,\n",
       " 'bbq.txt': 207,\n",
       " 'beapimp.hum': 207,\n",
       " 'beauty.tm': 209,\n",
       " 'beave.hum': 209,\n",
       " 'beer-g': 209,\n",
       " 'beer-gui': 209,\n",
       " 'beer.gam': 209,\n",
       " 'beer.hum': 209,\n",
       " 'beer.txt': 209,\n",
       " 'beerdiag.txt': 209,\n",
       " 'beergame.hum': 209,\n",
       " 'beergame.txt': 209,\n",
       " 'beerjesus.hum': 209,\n",
       " 'beershrm.fis': 209,\n",
       " 'beershrp.fis': 209,\n",
       " 'beerwarn.txt': 209,\n",
       " 'beesherb.txt': 209,\n",
       " 'beginn.ers': 209,\n",
       " 'berryeto.bev': 209,\n",
       " 'bhang.fun': 209,\n",
       " 'bhb.ill': 209,\n",
       " 'bible.txt': 209,\n",
       " 'bigpic1.hum': 209,\n",
       " 'billcat.hum': 209,\n",
       " 'bimg.prn': 209,\n",
       " 'bingbong.hum': 209,\n",
       " 'bitchcar.hum': 209,\n",
       " 'bitnet.txt': 209,\n",
       " 'blackadd': 209,\n",
       " 'blackapp.hum': 209,\n",
       " 'blackhol.hum': 209,\n",
       " 'blake7.lis': 209,\n",
       " 'blaster.hum': 209,\n",
       " 'bless.bc': 209,\n",
       " 'blkbean.txt': 209,\n",
       " 'blkbnsrc.vgn': 209,\n",
       " 'blood.txt': 209,\n",
       " 'blooprs1.asc': 209,\n",
       " 'bmdn01.txt': 209,\n",
       " 'bnb_quot.txt': 209,\n",
       " 'bnbeg2.4.txt': 209,\n",
       " 'bnbguide.txt': 209,\n",
       " 'boarchil.txt': 209,\n",
       " 'boatmemo.jok': 209,\n",
       " 'boe.hum': 209,\n",
       " 'bond-2.txt': 209,\n",
       " 'boneles2.txt': 209,\n",
       " 'booknuti.txt': 209,\n",
       " 'booze1.fun': 209,\n",
       " 'booze2.fun': 209,\n",
       " 'booze.fun': 209,\n",
       " 'bored.txt': 209,\n",
       " 'boston.geog': 209,\n",
       " 'bozo_tv.leg': 209,\n",
       " 'brainect.hum': 209,\n",
       " 'brdpudd.des': 209,\n",
       " 'bread.rcp': 209,\n",
       " 'bread.rec': 209,\n",
       " 'bread.txt': 209,\n",
       " 'breadpud.des': 209,\n",
       " 'bredcake.des': 209,\n",
       " 'brewing': 209,\n",
       " 'browneco.hum': 209,\n",
       " 'brownie.rec': 209,\n",
       " 'brush1.txt': 209,\n",
       " 'btaco.txt': 209,\n",
       " 'btcisfre.hum': 209,\n",
       " 'btscke01.des': 209,\n",
       " 'btscke02.des': 209,\n",
       " 'btscke03.des': 209,\n",
       " 'btscke04.des': 209,\n",
       " 'btscke05.des': 209,\n",
       " 'buffwing.pol': 209,\n",
       " 'bugbreak.hum': 209,\n",
       " 'bugs.txt': 209,\n",
       " 'buldrwho.txt': 209,\n",
       " 'bunacald.fis': 209,\n",
       " 'burrito.mea': 209,\n",
       " 'butcher.txt': 209,\n",
       " 'butstcod.fis': 209,\n",
       " 'butwrong.hum': 209,\n",
       " 'buzzword.hum': 209,\n",
       " 'bw-phwan.hat': 209,\n",
       " 'bw-summe.hat': 209,\n",
       " 'bw.txt': 209,\n",
       " 'byfb.txt': 209,\n",
       " 'c0dez.txt': 209,\n",
       " 'cabbage.txt': 209,\n",
       " 'caesardr.sal': 209,\n",
       " 'cake.rec': 209,\n",
       " 'calamus.hrb': 209,\n",
       " 'calculus.txt': 209,\n",
       " 'calif.hum': 209,\n",
       " 'calvin.txt': 209,\n",
       " 'cancer.rat': 209,\n",
       " 'candy.txt': 1352,\n",
       " 'candybar.fun': 1352,\n",
       " 'capital.txt': 967,\n",
       " 'caramels.des': 967,\n",
       " 'carowner.txt': 967,\n",
       " 'cars.txt': 967,\n",
       " 'cartoon.law': 967,\n",
       " 'cartoon.laws': 967,\n",
       " 'cartoon_.txt': 967,\n",
       " 'cartoon_laws.txt': 967,\n",
       " 'cartwb.son': 967,\n",
       " 'cast.lis': 967,\n",
       " 'catballs.hum': 967,\n",
       " 'catin.hat': 967,\n",
       " 'catranch.hum': 967,\n",
       " 'catstory.txt': 967,\n",
       " 'cbmatic.hum': 967,\n",
       " 'cereal.txt': 967,\n",
       " 'cform2.txt': 967,\n",
       " 'cgs_lst.txt': 967,\n",
       " 'chainltr.txt': 967,\n",
       " 'change.hum': 967,\n",
       " 'charity.hum': 967,\n",
       " 'cheapfar.hum': 967,\n",
       " 'cheapin.la': 967,\n",
       " 'chickenheadbbs.txt': 967,\n",
       " 'chickens.jok': 967,\n",
       " 'chickens.txt': 967,\n",
       " 'childhoo.jok': 967,\n",
       " 'childrenbooks.txt': 967,\n",
       " 'chili.txt': 967,\n",
       " 'chinese.txt': 967,\n",
       " 'chinesec.hum': 967,\n",
       " 'choco-ch.ips': 967,\n",
       " 'christop.int': 967,\n",
       " 'chung.iv': 967,\n",
       " 'chunnel.txt': 967,\n",
       " 'church.sto': 967,\n",
       " 'clancy.txt': 967,\n",
       " 'classicm.hum': 967,\n",
       " 'climbing.let': 967,\n",
       " 'cmu.share': 967,\n",
       " 'co-car.jok': 967,\n",
       " 'cockney.alp': 967,\n",
       " 'coffee.faq': 967,\n",
       " 'coffee.txt': 967,\n",
       " 'coffeebeerwomen.txt': 967,\n",
       " 'cogdis.txt': 967,\n",
       " 'coke1': 967,\n",
       " 'coke.fun': 967,\n",
       " 'coke.txt': 967,\n",
       " 'coke_fan.naz': 967,\n",
       " 'cokeform.txt': 967,\n",
       " 'coladrik.fun': 967,\n",
       " 'coladrik.txt': 967,\n",
       " 'cold.fus': 967,\n",
       " 'coldfake.hum': 967,\n",
       " 'collected_quotes.txt': 967,\n",
       " 'college.hum': 967,\n",
       " 'college.sla': 967,\n",
       " 'college.txt': 967,\n",
       " 'comic_st.gui': 967,\n",
       " 'commutin.jok': 967,\n",
       " 'commword.hum': 967,\n",
       " 'computer.txt': 967,\n",
       " 'comrevi1.hum': 967,\n",
       " 'conan.txt': 967,\n",
       " 'confucius_say.txt': 967,\n",
       " 'consp.txt': 967,\n",
       " 'contract.moo': 967,\n",
       " 'cookberk': 967,\n",
       " 'cookbkly.how': 967,\n",
       " 'cookie.1': 967,\n",
       " 'cooking.fun': 967,\n",
       " 'cooking.jok': 967,\n",
       " 'coollngo2.txt': 967,\n",
       " 'cooplaws': 967,\n",
       " 'cops.txt': 967,\n",
       " 'corporat.txt': 967,\n",
       " 'court.quips': 967,\n",
       " 'cowexplo.hum': 967,\n",
       " 'coyote.txt': 967,\n",
       " 'crazy.txt': 967,\n",
       " 'critic.txt': 967,\n",
       " 'crzycred.lst': 967,\n",
       " 'cuchy.hum': 967,\n",
       " 'cucumber.jok': 967,\n",
       " 'cucumber.txt': 967,\n",
       " 'cuisine.txt': 967,\n",
       " 'cultmov.faq': 967,\n",
       " 'curiousgeorgie.txt': 967,\n",
       " 'curry.hrb': 967,\n",
       " 'curry.txt': 967,\n",
       " 'curse.txt': 967,\n",
       " 'cybrtrsh.txt': 967,\n",
       " 'd-ned.hum': 967,\n",
       " 'dalive': 967,\n",
       " 'damiana.hrb': 967,\n",
       " 'dandwine.bev': 967,\n",
       " 'dark.suc': 967,\n",
       " 'dead2.txt': 967,\n",
       " 'dead3.txt': 967,\n",
       " 'dead4.txt': 967,\n",
       " 'dead5.txt': 967,\n",
       " 'dead-r': 967,\n",
       " 'deadlysins.txt': 967,\n",
       " 'deathhem.txt': 967,\n",
       " 'deep.txt': 967,\n",
       " 'defectiv.hum': 967,\n",
       " 'desk.txt': 967,\n",
       " 'deterior.hum': 967,\n",
       " 'devils.jok': 967,\n",
       " 'diesmurf.txt': 967,\n",
       " 'diet.txt': 967,\n",
       " 'dieter.txt': 967,\n",
       " 'dingding.hum': 967,\n",
       " 'dining.out': 967,\n",
       " 'dirtword.txt': 967,\n",
       " 'disaster.hum': 967,\n",
       " 'disclmr.txt': 967,\n",
       " 'disclym.txt': 967,\n",
       " 'doc-says.txt': 967,\n",
       " 'docdict.txt': 967,\n",
       " 'docspeak.txt': 967,\n",
       " 'doggun.sto': 967,\n",
       " 'donut.txt': 967,\n",
       " 'dover.poem': 967,\n",
       " 'draxamus.txt': 967,\n",
       " 'drinker.txt': 967,\n",
       " 'drinking.tro': 967,\n",
       " 'drinkrul.jok': 967,\n",
       " 'drinks.gui': 967,\n",
       " 'drinks.txt': 967,\n",
       " 'drive.txt': 967,\n",
       " 'dromes.txt': 967,\n",
       " 'druggame.hum': 967,\n",
       " 'drugshum.hum': 967,\n",
       " 'drunk.txt': 967,\n",
       " 'dthought.txt': 967,\n",
       " 'dubltalk.jok': 967,\n",
       " 'dym': 967,\n",
       " 'eandb.drx': 967,\n",
       " 'earp': 967,\n",
       " 'eatme.txt': 967,\n",
       " 'econridl.fun': 967,\n",
       " 'egg-bred.txt': 967,\n",
       " 'egglentl.vgn': 967,\n",
       " 'eggroll1.mea': 967,\n",
       " 'electric.txt': 967,\n",
       " 'element.jok': 967,\n",
       " 'elephant.fun': 967,\n",
       " 'elevator.fun': 967,\n",
       " 'empeval.txt': 967,\n",
       " 'engineer.hum': 967,\n",
       " 'english': 967,\n",
       " 'english.txt': 967,\n",
       " 'engmuffn.txt': 967,\n",
       " 'engrhyme.txt': 967,\n",
       " 'enlightenment.txt': 967,\n",
       " 'enquire.hum': 967,\n",
       " 'epi_.txt': 967,\n",
       " 'epi_bnb.txt': 967,\n",
       " 'epi_merm.txt': 967,\n",
       " 'epi_rns.txt': 967,\n",
       " 'epi_tton.txt': 967,\n",
       " 'epikarat.txt': 967,\n",
       " 'epiquest.txt': 967,\n",
       " 'episimp2.txt': 967,\n",
       " 'epitaph': 967,\n",
       " 'eskimo.nel': 967,\n",
       " 'exam.50': 967,\n",
       " 'excuse30.txt': 967,\n",
       " 'excuse.txt': 967,\n",
       " 'excuses.txt': 967,\n",
       " 'exidy.txt': 967,\n",
       " 'exylic.txt': 967,\n",
       " 'f_tang.txt': 967,\n",
       " 'facedeth.txt': 967,\n",
       " 'failure.txt': 967,\n",
       " 'fajitas.rcp': 967,\n",
       " 'farsi.phrase': 967,\n",
       " 'farsi.txt': 967,\n",
       " 'fartinfo.txt': 967,\n",
       " 'fartting.txt': 967,\n",
       " 'fascist.txt': 967,\n",
       " 'fbipizza.txt': 967,\n",
       " 'fearcola.hum': 967,\n",
       " 'fed.txt': 967,\n",
       " 'fegg!int.txt': 967,\n",
       " 'feggaqui.txt': 967,\n",
       " 'feggmagi.txt': 967,\n",
       " 'feista01.dip': 967,\n",
       " 'female.jok': 967,\n",
       " 'fiber.txt': 967,\n",
       " 'figure_1.txt': 967,\n",
       " 'filmgoof.txt': 967,\n",
       " 'films_gl.txt': 967,\n",
       " 'final-ex.txt': 967,\n",
       " 'finalexm.hum': 967,\n",
       " 'firecamp.txt': 967,\n",
       " 'fireplacein.txt': 967,\n",
       " 'firstaid.inf': 967,\n",
       " 'firstaid.txt': 967,\n",
       " 'fish.rec': 967,\n",
       " 'flattax.hum': 967,\n",
       " 'flowchrt': 967,\n",
       " 'flowchrt.txt': 967,\n",
       " 'flux_fix.txt': 967,\n",
       " 'focaccia.brd': 967,\n",
       " 'food': 967,\n",
       " 'foodtips': 967,\n",
       " 'footfun.hum': 967,\n",
       " 'forsooth.hum': 967,\n",
       " 'free-cof.fee': 967,\n",
       " 'freshman.hum': 967,\n",
       " 'freudonseuss.txt': 967,\n",
       " 'frogeye1.sal': 967,\n",
       " 'from.hum': 967,\n",
       " 'fuck!.txt': 967,\n",
       " 'fuckyou2.txt': 967,\n",
       " 'fudge.txt': 967,\n",
       " 'fusion.gal': 967,\n",
       " 'fusion.sup': 967,\n",
       " 'fwksfun.hum': 967,\n",
       " 'gack!.txt': 967,\n",
       " 'gaiahuma': 967,\n",
       " 'gameshow.txt': 967,\n",
       " 'ganamembers.txt': 967,\n",
       " 'garlpast.vgn': 967,\n",
       " 'gas.txt': 967,\n",
       " 'gd_alf.txt': 967,\n",
       " 'gd_drwho.txt': 967,\n",
       " 'gd_flybd.txt': 967,\n",
       " 'gd_frasr.txt': 967,\n",
       " 'gd_gal.txt': 967,\n",
       " 'gd_guide.txt': 967,\n",
       " 'gd_hhead.txt': 967,\n",
       " 'gd_liqtv.txt': 967,\n",
       " 'gd_maxhd.txt': 967,\n",
       " 'gd_ol.txt': 967,\n",
       " 'gd_ql.txt': 967,\n",
       " 'gd_sgrnd.txt': 967,\n",
       " 'gd_tznew.txt': 967,\n",
       " 'german.aut': 967,\n",
       " 'get.drunk.cheap': 967,\n",
       " 'ghostfun.hum': 967,\n",
       " 'ghostsch.hum': 967,\n",
       " 'gingbeer.txt': 967,\n",
       " 'girlspeak.txt': 967,\n",
       " 'godmonth.txt': 967,\n",
       " 'goforth.hum': 967,\n",
       " 'gohome.hum': 967,\n",
       " 'goldwatr.txt': 967,\n",
       " 'golnar.txt': 967,\n",
       " 'good.txt': 967,\n",
       " 'gotukola.hrb': 967,\n",
       " 'gown.txt': 967,\n",
       " 'grail.txt': 967,\n",
       " 'grammar.jok': 967,\n",
       " 'greenchi.txt': 967,\n",
       " 'grommet.hum': 967,\n",
       " 'grospoem.txt': 967,\n",
       " 'growth.txt': 967,\n",
       " 'gumbo.txt': 967,\n",
       " 'hack': 967,\n",
       " 'hack7.txt': 967,\n",
       " 'hackingcracking.txt': 967,\n",
       " 'hackmorality.txt': 967,\n",
       " 'hacktest.txt': 967,\n",
       " 'hamburge.nam': 967,\n",
       " 'hammock.hum': 967,\n",
       " 'hangover.txt': 967,\n",
       " 'happyhack.txt': 967,\n",
       " 'harmful.hum': 967,\n",
       " 'hate.hum': 967,\n",
       " 'hbo_spec.rev': 967,\n",
       " 'headlnrs': 967,\n",
       " 'hecomes.jok': 967,\n",
       " 'hedgehog.txt': 967,\n",
       " 'height.txt': 967,\n",
       " 'hell.jok': 967,\n",
       " 'hell.txt': 967,\n",
       " 'herb!.hum': 967,\n",
       " 'hermsys.txt': 967,\n",
       " 'heroic.txt': 967,\n",
       " 'hi.tec': 967,\n",
       " 'hierarch.txt': 967,\n",
       " 'highland.epi': 967,\n",
       " 'hilbilly.wri': 967,\n",
       " 'history2.oop': 967,\n",
       " 'hitchcoc.app': 967,\n",
       " 'hitchcok.txt': 967,\n",
       " 'hitler.59': 967,\n",
       " 'hitler.txt': 967,\n",
       " 'hitlerap.txt': 967,\n",
       " 'homebrew.txt': 967,\n",
       " 'homermmm.txt': 469,\n",
       " 'hoonsrc.txt': 469,\n",
       " 'hoosier.txt': 469,\n",
       " 'hop.faq': 469,\n",
       " 'horflick.txt': 469,\n",
       " 'horoscop.jok': 469,\n",
       " 'horoscop.txt': 469,\n",
       " 'horoscope.txt': 469,\n",
       " 'hotel.txt': 469,\n",
       " 'hotnnot.hum': 469,\n",
       " 'hotpeper.txt': 469,\n",
       " 'how2bgod.txt': 469,\n",
       " 'how2dotv.txt': 469,\n",
       " 'how.bugs.breakd': 469,\n",
       " 'how_to_i.pro': 469,\n",
       " 'howlong.hum': 469,\n",
       " 'hstlrtxt.txt': 469,\n",
       " 'htswfren.txt': 469,\n",
       " 'hum2': 469,\n",
       " 'humatra.txt': 469,\n",
       " 'humatran.jok': 469,\n",
       " 'humor9.txt': 469,\n",
       " 'humpty.dumpty': 469,\n",
       " 'iced.tea': 469,\n",
       " 'icm.hum': 469,\n",
       " 'idaho.txt': 469,\n",
       " 'idr2.txt': 469,\n",
       " 'imbecile.txt': 469,\n",
       " 'imprrisk.hum': 469,\n",
       " 'impurmat.hum': 469,\n",
       " 'incarhel.hum': 469,\n",
       " 'indgrdn.txt': 469,\n",
       " 'initials.rid': 469,\n",
       " 'inlaws1.txt': 469,\n",
       " 'inquirer.txt': 469,\n",
       " 'ins1': 469,\n",
       " 'insanity.hum': 469,\n",
       " 'insect1.txt': 469,\n",
       " 'insult': 469,\n",
       " 'insult.lst': 1167,\n",
       " 'insults1.txt': 810,\n",
       " 'insuranc.sty': 810,\n",
       " 'insure.hum': 810,\n",
       " 'interv.hum': 810,\n",
       " 'investi.hum': 810,\n",
       " 'iqtest': 810,\n",
       " 'iremember': 810,\n",
       " 'is_story.txt': 810,\n",
       " 'italoink.txt': 810,\n",
       " 'ivan.hum': 810,\n",
       " 'jac&tuu.hum': 810,\n",
       " 'jalapast.dip': 810,\n",
       " 'jambalay.pol': 810,\n",
       " 'japantv.txt': 810,\n",
       " 'japice.bev': 810,\n",
       " 'japrap.hum': 810,\n",
       " 'jargon.phd': 810,\n",
       " 'jason.fun': 810,\n",
       " 'jawgumbo.fis': 810,\n",
       " 'jawsalad.fis': 810,\n",
       " 'jayjay.txt': 810,\n",
       " 'jc-elvis.inf': 810,\n",
       " 'jeffie.heh': 810,\n",
       " 'jerky.rcp': 810,\n",
       " 'jimhood.txt': 810,\n",
       " 'johann': 810,\n",
       " 'jokeju07.txt': 810,\n",
       " 'jokes': 810,\n",
       " 'jokes1.txt': 810,\n",
       " 'jokes.txt': 788,\n",
       " 'jon.txt': 788,\n",
       " 'jrrt.riddle': 788,\n",
       " 'jungjuic.bev': 788,\n",
       " 'just2': 788,\n",
       " 'justify': 788,\n",
       " 'kaboom.hum': 788,\n",
       " 'kanalx.txt': 788,\n",
       " 'kashrut.txt': 788,\n",
       " 'kid2': 788,\n",
       " 'kid_diet.txt': 788,\n",
       " 'killer.hum': 788,\n",
       " 'killself.hum': 788,\n",
       " 'kilroy': 788,\n",
       " 'kilsmur.hum': 788,\n",
       " 'kloo.txt': 788,\n",
       " 'koans.txt': 788,\n",
       " 'la_times.hun': 788,\n",
       " 'labels.txt': 788,\n",
       " 'lampoon.jok': 788,\n",
       " 'languag.jok': 788,\n",
       " 'lansing.txt': 788,\n",
       " 'law.sch': 788,\n",
       " 'lawhunt.txt': 788,\n",
       " 'laws.txt': 788,\n",
       " 'lawskool.txt': 788,\n",
       " 'lawsuniv.hum': 788,\n",
       " 'lawyer.jok': 788,\n",
       " 'lawyers.txt': 788,\n",
       " 'lazarus.txt': 788,\n",
       " 'lbinter.hum': 788,\n",
       " 'leech.txt': 788,\n",
       " 'legal.hum': 788,\n",
       " 'let.go': 788,\n",
       " 'letgosh.txt': 788,\n",
       " 'letter.txt': 788,\n",
       " 'letter_f.sch': 788,\n",
       " 'letterbx.txt': 788,\n",
       " 'libraway.txt': 788,\n",
       " 'liceprof.sty': 788,\n",
       " 'lif&love.hum': 788,\n",
       " 'lifeimag.hum': 788,\n",
       " 'lifeinfo.hum': 788,\n",
       " 'lifeonledge.txt': 788,\n",
       " 'limerick.jok': 788,\n",
       " 'lines.jok': 788,\n",
       " 'lion.jok': 788,\n",
       " 'lion.txt': 788,\n",
       " 'lions.cat': 788,\n",
       " 'lipkovits.txt': 788,\n",
       " 'livnware.hum': 788,\n",
       " 'llamas.txt': 788,\n",
       " 'lll.hum': 788,\n",
       " 'llong.hum': 788,\n",
       " 'lobquad.hum': 788,\n",
       " 'looser.hum': 788,\n",
       " 'losers84.hum': 788,\n",
       " 'losers86.hum': 788,\n",
       " 'lost.txt': 788,\n",
       " 'lotsa.jok': 788,\n",
       " 'lozers': 788,\n",
       " 'lozerzon.hum': 788,\n",
       " 'lozeuser.hum': 788,\n",
       " 'lp-assoc.txt': 788,\n",
       " 'lucky.cha': 788,\n",
       " 'ludeinfo.hum': 788,\n",
       " 'ludeinfo.txt': 788,\n",
       " 'luggage.hum': 788,\n",
       " 'luvstory.txt': 788,\n",
       " 'luzerzo2.hum': 788,\n",
       " 'm0dzmen.hum': 788,\n",
       " 'macsfarm.old': 788,\n",
       " 'madhattr.jok': 788,\n",
       " 'madscrib.hum': 788,\n",
       " 'maecenas.hum': 788,\n",
       " 'mailfrag.hum': 788,\n",
       " 'makebeer.hum': 788,\n",
       " 'making_y.wel': 788,\n",
       " 'malechem.txt': 788,\n",
       " 'manager.txt': 788,\n",
       " 'manilla.hum': 788,\n",
       " 'manners.txt': 788,\n",
       " 'manspace.hum': 788,\n",
       " 'margos.txt': 788,\n",
       " 'marines.hum': 788,\n",
       " 'marriage.hum': 788,\n",
       " 'mash.hum': 788,\n",
       " 'math.1': 788,\n",
       " 'math.2': 788,\n",
       " 'math.far': 788,\n",
       " 'maxheadr': 788,\n",
       " 'mcd.txt': 788,\n",
       " 'mead.rcp': 788,\n",
       " 'meat2.txt': 788,\n",
       " 'meinkamp.hum': 788,\n",
       " 'mel.txt': 788,\n",
       " 'melodram.hum': 788,\n",
       " 'memo.hum': 788,\n",
       " 'memory.hum': 788,\n",
       " 'men&wome.txt': 788,\n",
       " 'mensroom.jok': 788,\n",
       " 'merry.txt': 788,\n",
       " 'miamadvi.hum': 788,\n",
       " 'miami.hum': 788,\n",
       " 'miamimth.txt': 788,\n",
       " 'middle.age': 788,\n",
       " 'mindvox': 788,\n",
       " 'minn.txt': 788,\n",
       " 'miranda.hum': 788,\n",
       " 'misc.1': 788,\n",
       " 'misery.hum': 788,\n",
       " 'missdish': 788,\n",
       " 'missheav.hum': 788,\n",
       " 'mitch.txt': 788,\n",
       " 'mlverb.hum': 788,\n",
       " 'modemwld.txt': 788,\n",
       " 'modest.hum': 788,\n",
       " 'modstup': 788,\n",
       " 'mog-history': 788,\n",
       " 'montoys.txt': 788,\n",
       " 'montpyth.hum': 788,\n",
       " 'moonshin': 788,\n",
       " 'moore.txt': 788,\n",
       " 'moose.txt': 788,\n",
       " 'moslem.txt': 788,\n",
       " 'mothers.txt': 788,\n",
       " 'motrbike.jok': 788,\n",
       " 'mov_rail.txt': 788,\n",
       " 'mowers.txt': 788,\n",
       " 'mr.rogers': 788,\n",
       " 'mrscienc.hum': 788,\n",
       " 'mrsfield': 788,\n",
       " 'msfields.txt': 788,\n",
       " 'msorrow': 788,\n",
       " 'mtm.hum': 788,\n",
       " 'mtv.asc': 788,\n",
       " 'mundane.v2': 788,\n",
       " 'murph.jok': 788,\n",
       " 'murphy.txt': 788,\n",
       " 'murphy_l.txt': 788,\n",
       " 'murphys.txt': 788,\n",
       " 'mutate.hum': 788,\n",
       " 'mydaywss.hum': 788,\n",
       " 'myheart.hum': 788,\n",
       " 'naivewiz.hum': 788,\n",
       " 'namaste.txt': 788,\n",
       " 'nameisreo.txt': 788,\n",
       " 'namm': 788,\n",
       " 'nasaglenn.txt': 788,\n",
       " 'necropls.txt': 788,\n",
       " 'netmask.txt': 788,\n",
       " 'netnews.10': 788,\n",
       " 'newcoke.txt': 788,\n",
       " 'newconst.hum': 788,\n",
       " 'newmex.hum': 788,\n",
       " 'news.hum': 788,\n",
       " 'nigel10.txt': 788,\n",
       " 'nigel.1': 788,\n",
       " 'nigel.2': 788,\n",
       " 'nigel.3': 788,\n",
       " 'nigel.4': 788,\n",
       " 'nigel.5': 788,\n",
       " 'nigel.6': 788,\n",
       " 'nigel.7': 788,\n",
       " 'nigel.10': 788,\n",
       " 'nihgel_8.9': 788,\n",
       " 'nintendo.jok': 788,\n",
       " 'normal.boy': 788,\n",
       " 'normalboy.txt': 788,\n",
       " 'normquot.txt': 469,\n",
       " 'nosuch_nasfic': 469,\n",
       " 'novel.hum': 469,\n",
       " 'nuke.hum': 469,\n",
       " 'nukeplay.hum': 469,\n",
       " 'nukewar.jok': 469,\n",
       " 'nukewar.txt': 469,\n",
       " 'nukwaste': 469,\n",
       " 'number': 469,\n",
       " 'number.killer': 469,\n",
       " 'number_k.ill': 469,\n",
       " 'nurds.hum': 469,\n",
       " 'nysucks.hum': 469,\n",
       " 'nzdrinks.txt': 469,\n",
       " 'o-ttalk.hum': 469,\n",
       " 'oakwood.txt': 469,\n",
       " 'oam-001.txt': 469,\n",
       " 'oam.nfo': 469,\n",
       " 'oasis': 469,\n",
       " 'oatbran.rec': 469,\n",
       " 'oculis.rcp': 469,\n",
       " 'odd_to.obs': 469,\n",
       " 'odearakk.hum': 469,\n",
       " 'office.txt': 469,\n",
       " 'ohandre.hum': 469,\n",
       " 'oilgluts.hum': 469,\n",
       " 'old.txt': 469,\n",
       " 'oldeng.hum': 469,\n",
       " 'oldtime.sng': 469,\n",
       " 'oldtime.txt': 469,\n",
       " 'oliver02.txt': 469,\n",
       " 'oliver.txt': 469,\n",
       " 'onan.txt': 469,\n",
       " 'one.par': 469,\n",
       " 'onetoone.hum': 469,\n",
       " 'onetotwo.hum': 469,\n",
       " 'ookpik.hum': 469,\n",
       " 'opinion.hum': 469,\n",
       " 'oracle.jok': 469,\n",
       " 'oranchic.pol': 469,\n",
       " 'orgfrost.bev': 469,\n",
       " 'ourfathr.txt': 469,\n",
       " 'outawork.erl': 469,\n",
       " 'outlimit.txt': 469,\n",
       " 'oxymoron.jok': 469,\n",
       " 'oxymoron.txt': 469,\n",
       " 'ozarks.hum': 469,\n",
       " 'p-law.hum': 469,\n",
       " 'packard.txt': 469,\n",
       " 'paddingurpapers.txt': 469,\n",
       " 'parabl.hum': 469,\n",
       " 'parades.hum': 469,\n",
       " 'parsnip.txt': 469,\n",
       " 'passage.hum': 469,\n",
       " 'passenge.sim': 469,\n",
       " 'pasta001.sal': 469,\n",
       " 'pat.txt': 469,\n",
       " 'pbcookie.des': 469,\n",
       " 'peanuts.txt': 469,\n",
       " 'peatchp.hum': 469,\n",
       " 'pecker.txt': 469,\n",
       " 'penisprt.txt': 469,\n",
       " 'penndtch': 217,\n",
       " 'pepper.txt': 217,\n",
       " 'pepsideg.txt': 193,\n",
       " 'petshop': 193,\n",
       " 'phony.hum': 193,\n",
       " 'phorse.hum': 193,\n",
       " 'phunatdi.ana': 193,\n",
       " 'phxbbs-m.txt': 193,\n",
       " 'pickup.lin': 193,\n",
       " 'pickup.txt': 193,\n",
       " 'pipespec.txt': 193,\n",
       " 'pizzawho.hum': 193,\n",
       " 'planeget.hum': 193,\n",
       " 'planetzero.txt': 193,\n",
       " 'poets.hum': 193,\n",
       " 'pol-corr.txt': 193,\n",
       " 'polemom.txt': 193,\n",
       " 'poli.tics': 193,\n",
       " 'poli_t.ics': 193,\n",
       " 'policpig.hum': 193,\n",
       " 'poll2res.hum': 193,\n",
       " 'polly.txt': 193,\n",
       " 'polly_.new': 193,\n",
       " 'poopie.txt': 193,\n",
       " 'popconc.hum': 193,\n",
       " 'popmach': 193,\n",
       " 'popmusi.hum': 193,\n",
       " 'post.nuc': 193,\n",
       " 'pot.txt': 193,\n",
       " 'potty.txt': 193,\n",
       " 'pournell.spo': 193,\n",
       " 'ppbeer.txt': 193,\n",
       " 'prac1.jok': 193,\n",
       " 'prac2.jok': 193,\n",
       " 'prac3.jok': 193,\n",
       " 'prac4.jok': 193,\n",
       " 'pracjoke.txt': 193,\n",
       " 'practica.txt': 513,\n",
       " 'prawblim.hum': 513,\n",
       " 'prayer.hum': 513,\n",
       " 'primes.jok': 513,\n",
       " 'princess.brd': 513,\n",
       " 'pro-fact.hum': 193,\n",
       " 'problem.txt': 193,\n",
       " 'progrs.gph': 193,\n",
       " 'proof.met': 193,\n",
       " 'prooftec.txt': 193,\n",
       " 'proposal.jok': 193,\n",
       " 'proudlyserve.txt': 193,\n",
       " 'prover.wisom': 193,\n",
       " 'prover_w.iso': 193,\n",
       " 'psalm23.txt': 193,\n",
       " 'psalm.reagan': 193,\n",
       " 'psalm_nixon': 193,\n",
       " 'psalm_re.aga': 193,\n",
       " 'psilaine.hum': 193,\n",
       " 'psych_pr.quo': 193,\n",
       " 'psycho.txt': 193,\n",
       " 'pukeprom.jok': 193,\n",
       " 'pun.txt': 193,\n",
       " 'pure.mat': 193,\n",
       " 'puzzle.spo': 193,\n",
       " 'puzzles.jok': 193,\n",
       " 'python_s.ong': 193,\n",
       " 'q.pun': 193,\n",
       " 'qttofu.vgn': 193,\n",
       " 'quack26.txt': 193,\n",
       " 'quantity.001': 193,\n",
       " 'quantum.jok': 193,\n",
       " 'quantum.phy': 193,\n",
       " 'quest.hum': 193,\n",
       " 'quick.jok': 193,\n",
       " 'quotes.bug': 193,\n",
       " 'quotes.jok': 193,\n",
       " 'quotes.txt': 193,\n",
       " 'quux_p.oem': 193,\n",
       " 'rabbit.txt': 193,\n",
       " 'racist.net': 193,\n",
       " 'radexposed.txt': 193,\n",
       " 'radiolaf.hum': 193,\n",
       " 'rapmastr.hum': 193,\n",
       " 'ratings.hum': 193,\n",
       " 'ratspit.hum': 193,\n",
       " 'raven.hum': 193,\n",
       " 'readme.bat': 193,\n",
       " 'reagan.hum': 193,\n",
       " 'realest.txt': 193,\n",
       " 'reasons.txt': 193,\n",
       " 'rec.por': 193,\n",
       " 'recepies.fun': 193,\n",
       " 'recip1.txt': 193,\n",
       " 'recipe.001': 193,\n",
       " 'recipe.002': 193,\n",
       " 'recipe.003': 193,\n",
       " 'recipe.004': 193,\n",
       " 'recipe.005': 193,\n",
       " 'recipe.006': 193,\n",
       " 'recipe.007': 193,\n",
       " 'recipe.008': 193,\n",
       " 'recipe.009': 193,\n",
       " 'recipe.010': 193,\n",
       " 'recipe.011': 193,\n",
       " 'recipe.012': 193,\n",
       " 'reconcil.hum': 193,\n",
       " 'record_.gap': 193,\n",
       " 'red-neck.jks': 193,\n",
       " 'reddwarf.sng': 193,\n",
       " 'reddye.hum': 193,\n",
       " 'rednecks.txt': 193,\n",
       " 'reeves.txt': 193,\n",
       " 'relative.ada': 193,\n",
       " 'religion.txt': 193,\n",
       " 'renored.txt': 193,\n",
       " 'renorthr.txt': 193,\n",
       " 'rent-a_cat': 193,\n",
       " 'rentals.hum': 193,\n",
       " 'repair.hum': 193,\n",
       " 'report.hum': 193,\n",
       " 'research.hum': 193,\n",
       " 'residncy.jok': 193,\n",
       " 'resolutn.txt': 193,\n",
       " 'resrch_p.hra': 193,\n",
       " 'resrch_phrase': 193,\n",
       " 'revolt.dj': 193,\n",
       " 'richbred.txt': 193,\n",
       " 'rinaldo.jok': 193,\n",
       " 'rinaldos.law': 193,\n",
       " 'rinaldos.txt': 193,\n",
       " 'ripoffpc.hum': 193,\n",
       " 'rns_bcl.txt': 193,\n",
       " 'rns_bwl.txt': 193,\n",
       " 'rns_ency.txt': 193,\n",
       " 'roach.asc': 193,\n",
       " 'roadpizz.txt': 193,\n",
       " 'robot.tes': 193,\n",
       " 'rocking.hum': 193,\n",
       " 'rockmus.hum': 193,\n",
       " 'sanshop.txt': 193,\n",
       " 'saveface.hum': 193,\n",
       " 'sawyer.txt': 193,\n",
       " 'scam.txt': 193,\n",
       " 'scratchy.txt': 198,\n",
       " 'seafood.txt': 198,\n",
       " 'seeds42.txt': 198,\n",
       " 'sf-zine.pub': 198,\n",
       " 'sfmovie.txt': 198,\n",
       " 'shameonu.hum': 198,\n",
       " 'shooters.txt': 399,\n",
       " 'shorties.jok': 399,\n",
       " 'shrink.news': 399,\n",
       " 'shuimai.txt': 198,\n",
       " 'shuttleb.hum': 198,\n",
       " 'signatur.jok': 198,\n",
       " 'sigs.txt': 198,\n",
       " 'silverclaws.txt': 198,\n",
       " 'simp.txt': 198,\n",
       " 'sinksub.txt': 198,\n",
       " 'skincat': 198,\n",
       " 'skippy.hum': 198,\n",
       " 'skippy.txt': 198,\n",
       " 'slogans.txt': 198,\n",
       " 'smackjok.hum': 198,\n",
       " 'smartass.txt': 198,\n",
       " 'smiley.txt': 198,\n",
       " 'smokers.txt': 198,\n",
       " 'smurf-03.txt': 198,\n",
       " 'smurf_co.txt': 198,\n",
       " 'smurfkil.hum': 198,\n",
       " 'smurfs.cc': 198,\n",
       " 'snapple.rum': 198,\n",
       " 'snipe.txt': 198,\n",
       " 'soccer.txt': 198,\n",
       " 'socecon.hum': 198,\n",
       " 'social.hum': 198,\n",
       " 'socks.drx': 198,\n",
       " 'solders.hum': 198,\n",
       " 'soleleer.hum': 198,\n",
       " 'solviets.hum': 198,\n",
       " 'some_hu.mor': 198,\n",
       " 'soporifi.abs': 198,\n",
       " 'sorority.gir': 198,\n",
       " 'spacever.hum': 198,\n",
       " 'spelin_r.ifo': 198,\n",
       " 'speling.msk': 198,\n",
       " 'spider.hum': 198,\n",
       " 'spoonlis.txt': 198,\n",
       " 'spydust.hum': 198,\n",
       " 'squids.gph': 198,\n",
       " 'st_silic.txt': 198,\n",
       " 'staff.txt': 198,\n",
       " 'stagline.txt': 198,\n",
       " 'standard.hum': 198,\n",
       " 'startrek.txt': 198,\n",
       " 'stereo.txt': 198,\n",
       " 'steroid.txt': 198,\n",
       " 'stone.hum': 198,\n",
       " 'strattma.txt': 198,\n",
       " 'stressman.txt': 198,\n",
       " 'strine.txt': 198,\n",
       " 'strsdiet.txt': 198,\n",
       " 'studentb.txt': 198,\n",
       " 'stuf10.txt': 198,\n",
       " 'stuf11.txt': 198,\n",
       " 'subb_lis.txt': 198,\n",
       " 'subrdead.hum': 198,\n",
       " 'suicide2.txt': 198,\n",
       " 'sungenu.hum': 198,\n",
       " 'supermar.rul': 198,\n",
       " 'sw_err.txt': 198,\n",
       " 'swearfrn.hum': 198,\n",
       " 'symbol.hum': 198,\n",
       " 'sysadmin.txt': 198,\n",
       " 'sysman.txt': 198,\n",
       " 't-10.hum': 198,\n",
       " 't-shirt.hum': 198,\n",
       " 't_zone.jok': 198,\n",
       " 'takenote.jok': 198,\n",
       " 'talebeat.hum': 198,\n",
       " 'talkbizr.txt': 198,\n",
       " 'taping.hum': 198,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_freq_doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df = pd.DataFrame(0, index=np.arange(len(corpus_words)), columns=doc_name)\n",
    "tf_df.index = corpus_words\n",
    "\n",
    "def termFreq_Double_norm():\n",
    "  doc_df_preprocessed = joblib.load('doc_df_preprocessed.joblib')\n",
    "  \n",
    "  for doc in doc_df_preprocessed.index:\n",
    "\n",
    "    for word in doc_df_preprocessed.loc[doc,'doc_token']:\n",
    "\n",
    "      tf_df.loc[word,doc] =0.5+0.5*(doc_df_preprocessed.loc[doc,'doc_token'].count(word)/max_freq_doc1[doc])\n",
    "  joblib.dump(tf_df,'termFreqDoubleNorm.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function call for term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# termFreq_Double_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# termFreq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# termFreq_binary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# termFreq_Log_norm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# termFreqRaw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133\n"
     ]
    }
   ],
   "source": [
    "num_of_doc = len(doc_df_preprocessed.index)\n",
    "print(num_of_doc)\n",
    "doc_token_list = joblib.load('doc_token_list.joblib')\n",
    "# doc_token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_of_doc = len(doc_df_preprocessed.index)\n",
    "print(num_of_doc)\n",
    "\n",
    "tf_df = joblib.load('termFreq.joblib')\n",
    "# corpus_words = tf_df.index\n",
    "\n",
    "def idf():\n",
    "    doc_token_list = joblib.load('doc_token_list.joblib')\n",
    "    idf_list ={}\n",
    "    for word in corpus_words:\n",
    "        count=0\n",
    "        for doc in doc_token_list:\n",
    "            # if word in doc:\n",
    "            count =  count + doc.count(word)\n",
    "        # print(count)\n",
    "        tf_df.loc[word,'idf'] = np.log(num_of_doc/(count+1))\n",
    "        \n",
    "    print(tf_df['idf'])\n",
    "\n",
    "idf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aviles</th>\n",
       "      <td>6.339477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unsteadily</th>\n",
       "      <td>6.339477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polio</th>\n",
       "      <td>5.423186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genovese</th>\n",
       "      <td>5.934012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creamsicle</th>\n",
       "      <td>5.423186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chest</th>\n",
       "      <td>2.675915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pirated</th>\n",
       "      <td>4.088185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>halleyuucp</th>\n",
       "      <td>6.339477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glycerine</th>\n",
       "      <td>5.646330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>platinum</th>\n",
       "      <td>5.423186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73606 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 idf\n",
       "aviles      6.339477\n",
       "unsteadily  6.339477\n",
       "polio       5.423186\n",
       "genovese    5.934012\n",
       "creamsicle  5.423186\n",
       "...              ...\n",
       "chest       2.675915\n",
       "pirated     4.088185\n",
       "halleyuucp  6.339477\n",
       "glycerine   5.646330\n",
       "platinum    5.423186\n",
       "\n",
       "[73606 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global idf_freq\n",
    "tf_df = joblib.load('termFreq.joblib')\n",
    "tf_df\n",
    "# for()\n",
    "idf_freq= tf_df[['idf']]\n",
    "idf_freq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # joblib.dump(idf_freq,'idf_freq.joblib')\n",
    "# joblib.dump(tf_df,'tf_df.joblib')\n",
    "# joblib.dump(corpus_words,'corpus_words.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(query):\n",
    "    df_raw = joblib.load('termFreqRaw.joblib')\n",
    "    df_binary = joblib.load('termFreqBinary.joblib')\n",
    "    df_logNorm = joblib.load('termFreqLogNorm.joblib')\n",
    "    df_termFreq =joblib.load('termFreq.joblib')\n",
    "    df_doubleNorm=joblib.load('termFreqDoubleNorm.joblib')\n",
    "\n",
    "    query_token_list = preProcess(query)\n",
    "\n",
    "    df_raw_vals = df_raw[df_raw.index.isin(query_token_list)].iloc[:,:-1]\n",
    "    df_raw_vals.iloc[:,:-1].mul(idf_freq['idf'])\n",
    "    \n",
    "    df_bin_vals = df_binary[df_binary.index.isin(query_token_list)].iloc[:,:-1]\n",
    "    df_bin_vals.iloc[:,:-1].mul(idf_freq['idf'])\n",
    "\n",
    "    df_lognorm_vals = df_logNorm[df_logNorm.index.isin(query_token_list)].iloc[:,:-1]\n",
    "    df_lognorm_vals.iloc[:,:-1].mul(idf_freq['idf'])\n",
    "    \n",
    "    df_termfreq_vals = df_termFreq[df_termFreq.index.isin(query_token_list)].iloc[:,:-1]\n",
    "    df_termfreq_vals.iloc[:,:-1].mul(idf_freq['idf'])\n",
    "    # print(df_termfreq_vals.sum(axis=0).sort_values(ascending=False).head(5).index)\n",
    "    \n",
    "    df_doubleNorm_vals = df_doubleNorm[df_doubleNorm.index.isin(query_token_list)].iloc[:,:-1]\n",
    "    df_doubleNorm_vals.iloc[:,:-1].mul(idf_freq['idf'])\n",
    "\n",
    "    # print(\"Binary IDF:\\n\",df_bin_vals.sum(axis=0).sort_values(ascending=False).head(5))\n",
    "    \n",
    "    # print(\"Double Normalization IDF:\\n\",df_doubleNorm_vals.sum(axis=0).sort_values(ascending=False).head(5))\n",
    "    # print(df_bin_vals)\n",
    "\n",
    "    \"\"\" individual terms k tf*idf has been made, \"\"\"\n",
    "    scores = pd.DataFrame(0, columns=['Top1','Top2','Top3','Top4','Top5'],index=['Binary','Raw count','Term frequency','Log normalization','Double normalization'])\n",
    "    scores[scores.index=='Binary']=df_bin_vals.sum(axis=0).sort_values(ascending=False).head(5).values\n",
    "    scores[scores.index=='Raw count']=df_raw_vals.sum(axis=0).sort_values(ascending=False).head(5)\n",
    "    scores[scores.index=='Term frequency']=df_termfreq_vals.sum(axis=0).sort_values(ascending=False).head(5)\n",
    "    scores[scores.index=='Log normalization']=df_lognorm_vals.sum(axis=0).sort_values(ascending=False).head(5)\n",
    "    scores[scores.index=='Double normalization']=df_doubleNorm_vals.sum(axis=0).sort_values(ascending=False).head(5)\n",
    "\n",
    "    results = pd.DataFrame(0, columns=['Top1','Top2','Top3','Top4','Top5'],index=['Binary','Raw count','Term frequency','Log normalization','Double normalization'])\n",
    "    \n",
    "    results[results.index=='Binary']=df_bin_vals.sum(axis=0).sort_values(ascending=False).head(5).index\n",
    "    results[results.index=='Raw count']=df_raw_vals.sum(axis=0).sort_values(ascending=False).head(5).index\n",
    "    results[results.index=='Term frequency']=df_termfreq_vals.sum(axis=0).sort_values(ascending=False).head(5).index\n",
    "    results[results.index=='Log normalization']=df_lognorm_vals.sum(axis=0).sort_values(ascending=False).head(5).index\n",
    "    results[results.index=='Double normalization']=df_doubleNorm_vals.sum(axis=0).sort_values(ascending=False).head(5).index\n",
    "    # print(scores)\n",
    "    return scores,results\n",
    "  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------- Testing query for TF-IDF -------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Raw count</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term frequency</th>\n",
       "      <td>0.006969</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.004053</td>\n",
       "      <td>0.003911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Log normalization</th>\n",
       "      <td>1.945910</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Double normalization</th>\n",
       "      <td>0.515544</td>\n",
       "      <td>0.510363</td>\n",
       "      <td>0.507576</td>\n",
       "      <td>0.505848</td>\n",
       "      <td>0.505181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Top1      Top2      Top3      Top4      Top5\n",
       "Binary                1.000000  1.000000  1.000000  1.000000  1.000000\n",
       "Raw count             6.000000  6.000000  5.000000  4.000000  4.000000\n",
       "Term frequency        0.006969  0.004926  0.004630  0.004053  0.003911\n",
       "Log normalization     1.945910  1.945910  1.791759  1.609438  1.609438\n",
       "Double normalization  0.515544  0.510363  0.507576  0.505848  0.505181"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores,results =TF_IDF('polio genovese chest')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>gameshow.txt</td>\n",
       "      <td>facedeth.txt</td>\n",
       "      <td>insults1.txt</td>\n",
       "      <td>cucumber.jok</td>\n",
       "      <td>barney.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Raw count</th>\n",
       "      <td>practica.txt</td>\n",
       "      <td>quest.hum</td>\n",
       "      <td>facedeth.txt</td>\n",
       "      <td>luzerzo2.hum</td>\n",
       "      <td>prac3.jok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term frequency</th>\n",
       "      <td>ghostfun.hum</td>\n",
       "      <td>age.txt</td>\n",
       "      <td>back1.txt</td>\n",
       "      <td>luzerzo2.hum</td>\n",
       "      <td>t_zone.jok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Log normalization</th>\n",
       "      <td>practica.txt</td>\n",
       "      <td>quest.hum</td>\n",
       "      <td>facedeth.txt</td>\n",
       "      <td>luzerzo2.hum</td>\n",
       "      <td>prac3.jok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Double normalization</th>\n",
       "      <td>quest.hum</td>\n",
       "      <td>prac3.jok</td>\n",
       "      <td>t_zone.jok</td>\n",
       "      <td>practica.txt</td>\n",
       "      <td>back1.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Top1          Top2          Top3          Top4  \\\n",
       "Binary                gameshow.txt  facedeth.txt  insults1.txt  cucumber.jok   \n",
       "Raw count             practica.txt     quest.hum  facedeth.txt  luzerzo2.hum   \n",
       "Term frequency        ghostfun.hum       age.txt     back1.txt  luzerzo2.hum   \n",
       "Log normalization     practica.txt     quest.hum  facedeth.txt  luzerzo2.hum   \n",
       "Double normalization     quest.hum     prac3.jok    t_zone.jok  practica.txt   \n",
       "\n",
       "                            Top5  \n",
       "Binary                barney.txt  \n",
       "Raw count              prac3.jok  \n",
       "Term frequency        t_zone.jok  \n",
       "Log normalization      prac3.jok  \n",
       "Double normalization   back1.txt  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_csv(\"./IR-assignment-2-data.txt\",sep = \" \",header=None)\n",
    "data\n",
    "final_data = data[data[1]==\"qid:4\"][[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     relevance_score\n",
       "0                  0\n",
       "1                  0\n",
       "2                  0\n",
       "3                  0\n",
       "4                  1\n",
       "..               ...\n",
       "98                 0\n",
       "99                 1\n",
       "100                2\n",
       "101                1\n",
       "102                0\n",
       "\n",
       "[103 rows x 1 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.rename(columns={final_data.columns[0]:\"relevance_score\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19893497375938370599826047614905329896936840170566570588205180312704857992695193482412686565431050240000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Placing a less relevant document above a more relevant documnet results in decreased DCG. \n",
    "So we'll make a file that contains the ideal ordering according to relevance\"\"\"\n",
    "def makeFile(dataset,data):\n",
    "    temp = dataset[dataset.index.isin(data.index)]\n",
    "    temp1= temp.sort_values(by=[0],ascending=False)\n",
    "    np.savetxt('qid_4_max.txt', temp1.values, fmt='%s', delimiter=\" \")\n",
    "\n",
    "    # print(temp) \n",
    "    \n",
    "\n",
    "makeFile(data,final_data)\n",
    "\n",
    "\"\"\"if rel3 has n files then we have to rank all of them so it will be n! AND rel2! and so on\"\"\"\n",
    "def countFiles(data):\n",
    "    counts =data[0].value_counts()\n",
    "    count_rel0 = counts[0]\n",
    "    count_rel1 = counts[1]\n",
    "    count_rel2 = counts[2]\n",
    "    count_rel3 = counts[3]\n",
    "    total_files = math.factorial(count_rel0)*math.factorial(count_rel1)*math.factorial(count_rel2)*math.factorial(count_rel3)\n",
    "    # print(data[0].value_counts())\n",
    "    print(total_files)\n",
    "\n",
    "countFiles(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "7    3\n",
       "76   2\n",
       "40   2\n",
       "36   2\n",
       "90   2\n",
       "..  ..\n",
       "44   0\n",
       "43   0\n",
       "42   0\n",
       "41   0\n",
       "102  0\n",
       "\n",
       "[103 rows x 1 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data\n",
    "ideal_data = final_data.sort_values(by=[0],ascending=False)\n",
    "ideal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    1\n",
       "..  ..\n",
       "98   0\n",
       "99   1\n",
       "100  2\n",
       "101  1\n",
       "102  0\n",
       "\n",
       "[103 rows x 1 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.550247459532576\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def DCG(data,length):\n",
    "    # final_data['DCG'] = final_data.apply(lambda row: row[0])\n",
    "    dcg= data[0][1]\n",
    "    # print(data[0][4])\n",
    "    # print(dcg)\n",
    "    for i in range(1,length):\n",
    "        curr = data.iloc[i,0]/math.log2(i+1)\n",
    "        dcg=dcg + curr\n",
    "    return dcg\n",
    "\n",
    "print(DCG(final_data,len(final_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max DCG is: 12.550247459532576\n",
      "nDCG at 50: 0.41082175342157357\n",
      "nDCG whole Dataset: 0.6976332021320715\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"MaxDCG\"\"\"\n",
    "maxDCG = DCG(final_data,len(final_data))\n",
    "\n",
    "\"\"\"nDCG for 50 values\"\"\"\n",
    "trueDCG50 = DCG(final_data,51)\n",
    "idealDCG50 = DCG(ideal_data,51)\n",
    "\n",
    "\"\"\"nDCG for whole dataset\"\"\"\n",
    "trueDCG = DCG(final_data,len(final_data))\n",
    "idealDCG = DCG(ideal_data,len(ideal_data))\n",
    "\n",
    "nDCG50 = trueDCG50/idealDCG50\n",
    "nDCG = trueDCG/idealDCG\n",
    "\n",
    "print(\"Max DCG is: {}\".format(maxDCG))\n",
    "print(\"nDCG at 50: {}\".format(nDCG50))\n",
    "print(\"nDCG whole Dataset: {}\".format(nDCG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/vvfw94kd5d539xp9whkr42s80000gn/T/ipykernel_27117/780956921.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['tf']= data[76].str.slice(3,len(data[76]))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>90.53171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>538.388954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>88.171761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>144.564444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>142.589323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>70.460443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>270.13233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2</td>\n",
       "      <td>296.023694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1</td>\n",
       "      <td>528.520116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>84.625987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0          tf\n",
       "0    0    90.53171\n",
       "1    0  538.388954\n",
       "2    0   88.171761\n",
       "3    0  144.564444\n",
       "4    1  142.589323\n",
       "..  ..         ...\n",
       "98   0   70.460443\n",
       "99   1   270.13233\n",
       "100  2  296.023694\n",
       "101  1  528.520116\n",
       "102  0   84.625987\n",
       "\n",
       "[103 rows x 2 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"feature 75\"\"\"\n",
    "\n",
    "global prec_recall_data\n",
    "prec_recall_data =pd.DataFrame()\n",
    "def tf(data):\n",
    "    data = data[data[1]=='qid:4']\n",
    "    data['tf']= data[76].str.slice(3,len(data[76]))\n",
    "    # print(data['tf'])\n",
    "    prec_recall_data = data[[0,'tf']]\n",
    "    # print(prec_recall_data )\n",
    "    return prec_recall_data\n",
    "prec_recall_data=tf(data)\n",
    "prec_recall_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def precision_recall():\n",
    "    \"\"\"non 0 rel are relevant\"\"\"\n",
    "    # total= len(prec_recall_data)\n",
    "    counts = prec_recall_data[0].value_counts()\n",
    "    rel_docs = sum(counts[1:])\n",
    "    # not_rel_docs = counts[0]\n",
    "    retieved_rel_docs=0\n",
    "    ranked_data = prec_recall_data.sort_values(by=['tf'],ascending=False)\n",
    "    precision=[]\n",
    "    recall=[]\n",
    "    # print(counts)\n",
    "    for i in range(0,len(ranked_data)):\n",
    "        if(ranked_data.iloc[i,0]!=0):\n",
    "            # print(ranked_data.iloc[i,:])\n",
    "            retieved_rel_docs=retieved_rel_docs+1\n",
    "        precision.append(retieved_rel_docs/(i+1))\n",
    "        recall.append(retieved_rel_docs/rel_docs)\n",
    "    # print(ranked_data)    \n",
    "    plt.plot(recall, precision,color=\"tab:orange\")\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision - Recall Curve')\n",
    "    plt.show()\n",
    "    # print(rel_docs)\n",
    "precision_recall()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
